{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.interpolate import griddata\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.utils.class_weight import compute_sample_weight\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.layers import Input, BatchNormalization\n",
    "\n",
    "tf.random.set_seed(42)\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.offline as pyo\n",
    "from plotly import subplots\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = pd.read_csv(\"../../data/client.csv\")\n",
    "ePrices = pd.read_csv(\"../../data/electricity_prices.csv\")\n",
    "gasPrices = pd.read_csv(\"../../data/gas_prices.csv\")\n",
    "train = pd.read_csv(\"../../data/train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weatherPredInt = pd.read_csv(\"interpolPredWeather.csv\")\n",
    "weatherHistInt = pd.read_csv(\"../histWeatherSnowCover.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# process data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### merge client & train, add business feat & interpolate daylight savings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "producing = train.loc[train.is_consumption == 0]\n",
    "consuming = train.loc[train.is_consumption == 1]\n",
    "train = pd.merge(producing.drop('is_consumption',axis = 1), consuming.drop('is_consumption',axis = 1),on=['data_block_id','prediction_unit_id','datetime','county','is_business','product_type'], how='outer',suffixes=('_prod', '_cons'))\n",
    "del producing, consuming\n",
    "print(train.shape)\n",
    "\n",
    "clientsTime = pd.merge(train, client, on=['county','is_business','product_type','data_block_id'], how='inner')\n",
    "\n",
    "clientsTime['datetime'] = pd.to_datetime(clientsTime['datetime'])\n",
    "clientsTime['yearday'] = clientsTime['datetime'].dt.day_of_year\n",
    "clientsTime['weekday'] = clientsTime['datetime'].dt.day_of_week\n",
    "clientsTime['month'] = clientsTime['datetime'].dt.month\n",
    "clientsTime['monthday'] = clientsTime['datetime'].dt.day\n",
    "clientsTime['year'] = clientsTime['datetime'].dt.year\n",
    "\n",
    "unique_pairs = list(set(zip(clientsTime['is_business'], clientsTime[ 'product_type'])))\n",
    "pair_index_dict = {pair: index for index, pair in enumerate(unique_pairs)}\n",
    "clientsTime['business_prodType'] = list(map(pair_index_dict.get, zip(clientsTime['is_business'], clientsTime['product_type'])))\n",
    "\n",
    "unique_pairs_cust = list(set(zip(clientsTime['is_business'], clientsTime[ 'product_type'], clientsTime['county'], clientsTime['eic_count'],clientsTime['installed_capacity'])))\n",
    "pair_index_dict = {pair: index for index, pair in enumerate(unique_pairs_cust)}\n",
    "clientsTime['ind_customer_id'] = list(map(pair_index_dict.get, zip(clientsTime['is_business'], clientsTime['product_type'], clientsTime['county'], clientsTime['eic_count'],clientsTime['installed_capacity'])))\n",
    "\n",
    "# interpolate daylight savings\n",
    "clientsTime = clientsTime.interpolate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### get holidays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import holidays\n",
    "from datetime import date\n",
    "\n",
    "us_holidays = holidays.EE()  # this is a dict\n",
    "\n",
    "clientsTime['holiday'] = clientsTime['datetime'].apply(lambda s : s in us_holidays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clientsTime['no_workday'] = ((clientsTime['holiday']) | (clientsTime['weekday'] > 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### prepare electric prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ePrices.euros_per_mwh.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ePrices['forecast_date'] = pd.to_datetime(ePrices['forecast_date'])\n",
    "\n",
    "# Set 'timestamp' as the index\n",
    "ePrices.set_index('forecast_date', inplace=True)\n",
    "\n",
    "# Resample to fill missing hours\n",
    "df_resampled = ePrices.resample('1H').asfreq()\n",
    "\n",
    "# Linearly interpolate missing values\n",
    "ePrices = df_resampled.interpolate(method='linear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ePrices['euros_per_mwh'] = ePrices.euros_per_mwh.replace(4000.0, np.nan)\n",
    "\n",
    "# Resample to fill missing hours\n",
    "df_resampled = ePrices.resample('1H').asfreq()\n",
    "\n",
    "# Linearly interpolate missing values\n",
    "ePrices = df_resampled.interpolate(method='linear')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# write data into python arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clientsTime, weatherPredInt, weatherHistInt, ePrices, gasPrices\n",
    "\n",
    "featPredWeather = [\n",
    "        #'latitude', 'longitude', \n",
    "        'County', #'forecast_datetime',\n",
    "       'hours_ahead',\n",
    "        'data_block_id', #'origin_datetime', \n",
    "       'temperatureint',\n",
    "       'dewpointint', 'cloudcover_highint', 'cloudcover_lowint',\n",
    "       'cloudcover_midint', 'cloudcover_totalint',\n",
    "       '10_metre_u_wind_componentint', '10_metre_v_wind_componentint',\n",
    "       'direct_solar_radiationint', 'surface_solar_radiation_downwardsint',\n",
    "       'snowfallint', 'total_precipitationint', \n",
    "       #'sunrise', 'sunset',\n",
    "       #'daylight', 'minDaylight'\n",
    "       ]\n",
    "\n",
    "predWeatherComplete = weatherPredInt[featPredWeather]\n",
    "del weatherPredInt\n",
    "\n",
    "featHistWeather = [\n",
    "       #'latitude', 'longitude', \n",
    "       'County',\n",
    "       #'datetime', \n",
    "       'data_block_id', \n",
    "       'temperatureint', 'dewpointint', 'rainint',\n",
    "       'snowfallint', 'surface_pressureint', 'cloudcover_totalint',\n",
    "       'cloudcover_lowint', 'cloudcover_midint', 'cloudcover_highint',\n",
    "       'windspeed_10mint', 'winddirection_10mint', 'shortwave_radiationint',\n",
    "       'direct_solar_radiationint', 'diffuse_radiationint', 'meltingInCM',\n",
    "       'snowHeightFlux', 'snowcover'\n",
    "]\n",
    "histWeatherComplete = weatherHistInt[featHistWeather]\n",
    "del weatherHistInt\n",
    "\n",
    "# can't use client id as an axis in data because it's not constant!\n",
    "y = np.zeros((clientsTime.shape[0],2))\n",
    "y_indexes = np.zeros((clientsTime.shape[0],2)) # dataframe indexes in the end, not important for now\n",
    "index_y = 0\n",
    "\n",
    "customers = []\n",
    "\n",
    "constValsArray = np.zeros((1,17))\n",
    "targetsArray = np.zeros((1,24,2))\n",
    "oldTargetsArray = np.zeros((1,24,2)) # targets from t-1\n",
    "ePricesArray = np.zeros((1,24,1))\n",
    "\n",
    "nFeatHistWeather = histWeatherComplete.shape[1]-1\n",
    "histWeatherArray = np.zeros((1,24,nFeatHistWeather))\n",
    "nFeatPredWeather = predWeatherComplete.shape[1]-1\n",
    "predWeatherArray = np.zeros((1,24,nFeatPredWeather))\n",
    "dataBlockIdArray = np.zeros((1))\n",
    "\n",
    "# loop over customer, append all data cycles for each customer\n",
    "for customerId in clientsTime.ind_customer_id.unique():\n",
    "    customerSlice = clientsTime.loc[clientsTime.ind_customer_id == customerId]\n",
    "\n",
    "    y_cons = 0\n",
    "    y_prod = 0\n",
    "    for dataBlockId in customerSlice.data_block_id.unique():\n",
    "        timeSlice = customerSlice.loc[customerSlice.data_block_id == dataBlockId]\n",
    "        gasSlice = gasPrices.loc[gasPrices.data_block_id == dataBlockId]\n",
    "        eSlice   = ePrices.loc[ePrices.data_block_id == dataBlockId]\n",
    "\n",
    "        if len(timeSlice.yearday.unique()) > 1:\n",
    "            print(timeSlice.yearday.unique())\n",
    "\n",
    "        # const values\n",
    "        yearday = timeSlice.yearday.unique()[0]\n",
    "        weekday = timeSlice.weekday.unique()[0]\n",
    "        month   = timeSlice.month.unique()[0]\n",
    "        monthday = timeSlice.monthday.unique()[0]\n",
    "        year = timeSlice.year.unique()[0]\n",
    "\n",
    "        holiday = timeSlice.holiday.unique()[0]\n",
    "        noWorkDay = timeSlice.no_workday.unique()[0]\n",
    "\n",
    "        county       = customerSlice.county.unique()[0]\n",
    "        is_business  = customerSlice.is_business.unique()[0]\n",
    "        product_type = customerSlice.product_type.unique()[0]\n",
    "        prediction_unit_id = customerSlice.prediction_unit_id.unique()[0] #should be redundant\n",
    "        eic_count          = customerSlice.eic_count.unique()[0]\n",
    "        installed_capacity = customerSlice.installed_capacity.unique()[0]\n",
    "    \n",
    "        business_prodType = customerSlice.business_prodType.unique()[0]\n",
    "        ind_customer_id   = customerSlice.ind_customer_id.unique()[0]\n",
    "\n",
    "        \n",
    "\n",
    "        lowest_price_per_mwh = gasSlice.lowest_price_per_mwh.iloc[0]\n",
    "        highest_price_per_mwh = gasSlice.highest_price_per_mwh.iloc[0]\n",
    "        euros_per_mwh = eSlice['euros_per_mwh'].to_numpy()\n",
    "        if euros_per_mwh.shape[0] == 23:\n",
    "            print(dataBlockId, customerId)\n",
    "\n",
    "        # feed the old targets per customer\n",
    "        new_row = np.zeros((1,24,2))\n",
    "        new_row[:,:,0] = y_cons\n",
    "        new_row[:,:,1] = y_prod\n",
    "        oldTargetsArray = np.concatenate((oldTargetsArray, new_row), axis=0)\n",
    "\n",
    "        y_cons = timeSlice['target_cons']\n",
    "        y_prod = timeSlice['target_prod']\n",
    "\n",
    "        histWeather = histWeatherComplete.loc[(histWeatherComplete.data_block_id == dataBlockId) & (histWeatherComplete.County == county)]\n",
    "        predWeather = predWeatherComplete.loc[(predWeatherComplete.data_block_id == dataBlockId) & (predWeatherComplete.County == county)]\n",
    "\n",
    "        histWeather = histWeather.drop('data_block_id', axis = 1)\n",
    "        predWeather = predWeather.drop('data_block_id', axis = 1)\n",
    "\n",
    "\n",
    "        if y_cons.shape[0] == 23:\n",
    "            print(dataBlockId, customerId)\n",
    "\n",
    "        new_row = np.array([county, is_business, product_type, prediction_unit_id, eic_count, installed_capacity, \n",
    "        business_prodType,ind_customer_id,lowest_price_per_mwh,highest_price_per_mwh,\n",
    "        yearday,weekday,month,monthday,year,holiday,noWorkDay\n",
    "        ])\n",
    "        constValsArray = np.vstack((constValsArray, new_row))\n",
    "\n",
    "        new_row = np.zeros((1,24,2))\n",
    "        new_row[:,:,0] = y_cons\n",
    "        new_row[:,:,1] = y_prod\n",
    "        targetsArray = np.concatenate((targetsArray, new_row), axis=0)\n",
    "\n",
    "        new_row = np.zeros((1,24,1))\n",
    "        new_row[0,:,0] = euros_per_mwh\n",
    "        ePricesArray = np.concatenate((ePricesArray, new_row), axis=0)\n",
    "\n",
    "        new_row = np.zeros((1,24,nFeatHistWeather)) #without datablock id\n",
    "        new_row[:,:,:] = histWeather\n",
    "        histWeatherArray = np.concatenate((histWeatherArray, new_row), axis=0)\n",
    "\n",
    "        new_row = np.zeros((1,24,nFeatPredWeather)) # withotu datablock id\n",
    "        new_row[:,:,:] = predWeather\n",
    "        predWeatherArray = np.concatenate((predWeatherArray, new_row), axis=0)\n",
    "\n",
    "        dataBlockIdArray = np.concatenate((dataBlockIdArray, np.array([dataBlockId])))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove first rows\n",
    "targetsArray     = targetsArray[1:,:,:]\n",
    "oldTargetsArray  = oldTargetsArray[1:,:,:]\n",
    "constValsArray   = constValsArray[1:,:]\n",
    "ePricesArray     = ePricesArray[1:,:,:]\n",
    "histWeatherArray = histWeatherArray[1:,:,:]\n",
    "predWeatherArray = predWeatherArray[1:,:,:]\n",
    "dataBlockIdArray = dataBlockIdArray[1:]\n",
    "print(targetsArray.shape,oldTargetsArray.shape, constValsArray.shape, ePricesArray.shape,histWeatherArray.shape, predWeatherArray.shape, dataBlockIdArray.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez('data_arrays_absTarget_oldTarget.npz', arr1=targetsArray, arr2=constValsArray, arr3=ePricesArray, arr4 = histWeatherArray, arr5=predWeatherArray, arr6=dataBlockIdArray, arr7=oldTargetsArray)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load data & preselect data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16 18 19\n"
     ]
    }
   ],
   "source": [
    "loaded_data = np.load('data_arrays_absTarget_oldTarget.npz')\n",
    "\n",
    "# Access individual arrays by their keys\n",
    "targetsAbs = loaded_data['arr1']\n",
    "constValsArray = loaded_data['arr2']\n",
    "ePricesArray = loaded_data['arr3']\n",
    "histWeatherArray = loaded_data['arr4']\n",
    "predWeatherArray = loaded_data['arr5']\n",
    "dataBlockIdArray = loaded_data['arr6']\n",
    "oldTargetsAbs = loaded_data['arr7']\n",
    "\n",
    "\n",
    "featPredWeather = [\n",
    "        'County',\n",
    "       'hours_ahead',\n",
    "       # 'data_block_id',\n",
    "       'temperatureint',\n",
    "       'dewpointint', 'cloudcover_highint', 'cloudcover_lowint',\n",
    "       'cloudcover_midint', \n",
    "       'cloudcover_totalint',\n",
    "       '10_metre_u_wind_componentint', '10_metre_v_wind_componentint',\n",
    "       'direct_solar_radiationint', 'surface_solar_radiation_downwardsint',\n",
    "       'snowfallint', 'total_precipitationint', \n",
    "       ]\n",
    "\n",
    "#correct hours\n",
    "predWeatherArray[:,:,1] = predWeatherArray[:,:,1] - 24\n",
    "\n",
    "# add sin of hours\n",
    "sinDay = np.reshape(np.sin((2*predWeatherArray[:,:,1] - 24)*np.pi/24),(predWeatherArray.shape[0],predWeatherArray.shape[1],1))\n",
    "predWeatherArray = np.concatenate((predWeatherArray, sinDay), axis=2)\n",
    "featPredWeather = featPredWeather + ['sinDay']\n",
    "\n",
    "cosDay = np.reshape(np.cos((2*predWeatherArray[:,:,1] - 24)*np.pi/24),(predWeatherArray.shape[0],predWeatherArray.shape[1],1))\n",
    "predWeatherArray = np.concatenate((predWeatherArray, cosDay), axis=2)\n",
    "featPredWeather = featPredWeather + ['cosDay']\n",
    "nPredFeat = predWeatherArray.shape[2]\n",
    "\n",
    "\n",
    "featConst =['county', 'is_business', 'product_type', 'prediction_unit_id', 'eic_count', 'installed_capacity', 'business_prodType', 'ind_customer_id',\n",
    "'lowest_price_per_mwh','highest_price_per_mwh','yearday','weekday','month','monthday','year','holiday','no_workday'\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "# add sin of year day\n",
    "yeardayIdx = 10\n",
    "CapIdx = 5\n",
    "sinYear = np.reshape(np.sin(2*constValsArray[:,yeardayIdx]*np.pi/365),(constValsArray.shape[0],1))\n",
    "constValsArray = np.concatenate((constValsArray, sinYear), axis=1)\n",
    "featConst = featConst + ['sinYearDay']\n",
    "\n",
    "cosYear = np.reshape(np.cos(2*constValsArray[:,yeardayIdx]*np.pi/365),(constValsArray.shape[0],1))\n",
    "constValsArray = np.concatenate((constValsArray, cosYear), axis=1)\n",
    "featConst = featConst + ['cosYearDay']\n",
    "\n",
    "nConst = constValsArray.shape[1]\n",
    "\n",
    "\n",
    "\n",
    "featHistWeather = [\n",
    "       'County', \n",
    "       #'data_block_id', \n",
    "       'temperatureint', 'dewpointint', 'rainint',\n",
    "       'snowfallint', 'surface_pressureint', 'cloudcover_totalint',\n",
    "       'cloudcover_lowint', 'cloudcover_midint', 'cloudcover_highint',\n",
    "       'windspeed_10mint', 'winddirection_10mint', 'shortwave_radiationint',\n",
    "       'direct_solar_radiationint', 'diffuse_radiationint', 'meltingInCM',\n",
    "       'snowHeightFlux', 'snowcover'\n",
    "]\n",
    "#featHistWeatherKeepIdx = [1,10]\n",
    "#histWeatherArray=histWeatherArray[:,:,featHistWeatherKeepIdx]\n",
    "\n",
    "#featHistWeather = [featHistWeather[i] for i in featHistWeatherKeepIdx]\n",
    "nHistFeat = histWeatherArray.shape[2]\n",
    "\n",
    "print(nPredFeat,nHistFeat,nConst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 18 13\n"
     ]
    }
   ],
   "source": [
    "featPredWeatherKeepIdx = [14,15] #only keep cos/sin day features\n",
    "predWeatherArray=predWeatherArray[:,:,featPredWeatherKeepIdx]\n",
    "featPredWeather = [featPredWeather[i] for i in featPredWeatherKeepIdx]\n",
    "nPredFeat = predWeatherArray.shape[2]\n",
    "\n",
    "#3,4 (prediction unit id, eic count)\n",
    "#7 ind cust id\n",
    "# 10 yearday\n",
    "# 13 monthday\n",
    "# 14 year\n",
    "# 15 holiday\n",
    "featConstKeepIdx = [0,1,2,5,6,8,9,10,11,12,16,17,18]\n",
    "constValsArray=constValsArray[:,featConstKeepIdx]\n",
    "featConst = [featConst[i] for i in featConstKeepIdx]\n",
    "nConst = constValsArray.shape[1]\n",
    "\n",
    "#featHistWeatherKeepIdx = [1,10]\n",
    "#histWeatherArray=histWeatherArray[:,:,featHistWeatherKeepIdx]\n",
    "#featHistWeather = [featHistWeather[i] for i in featHistWeatherKeepIdx]\n",
    "nHistFeat = histWeatherArray.shape[2]\n",
    "print(nPredFeat,nHistFeat,nConst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# build a model & train it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### split data based on time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainSplit = int(max(dataBlockIdArray)*0.7)\n",
    "mask = dataBlockIdArray < trainSplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffledArr = [constValsArray,ePricesArray,histWeatherArray,predWeatherArray,targetsAbs,oldTargetsAbs[:,:,0],dataBlockIdArray]\n",
    "\n",
    "\n",
    "#X_train = [shuffledArr[0][mask],shuffledArr[1][mask],shuffledArr[2][mask],shuffledArr[3][mask],shuffledArr[5][mask]]\n",
    "X_train = [shuffledArr[0][mask],shuffledArr[1][mask],shuffledArr[3][mask],shuffledArr[5][mask]]\n",
    "y_trainAbs = shuffledArr[4][mask]\n",
    "\n",
    "#X_test = [shuffledArr[0][~mask],shuffledArr[1][~mask],shuffledArr[2][~mask],shuffledArr[3][~mask],shuffledArr[5][~mask]]\n",
    "X_test = [shuffledArr[0][~mask],shuffledArr[1][~mask],shuffledArr[3][~mask],shuffledArr[5][~mask]]\n",
    "y_testAbs = shuffledArr[4][~mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### shuffle training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "index_array = np.arange(X_train[0].shape[0])\n",
    "if 1: #not shuffle!!\n",
    "    np.random.shuffle(index_array)\n",
    "\n",
    "arrays = [X_train[0],X_train[1],X_train[2],X_train[3],y_trainAbs]\n",
    "[X_train[0],X_train[1],X_train[2],X_train[3],y_trainAbs] = [arr[index_array] for arr in arrays]\n",
    "del arrays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### calculate class weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Calculate class weights on target\n",
    "class_weights = compute_sample_weight(class_weight='balanced', y=y_train[:,:,0]) #, X=[X_train[0][:,[yeardayIdx,5,6]]])\n",
    "\n",
    "class_weight_dict = dict(enumerate(class_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# based on capacity (3) or prod business type\n",
    "class_weights = compute_sample_weight(class_weight='balanced', y=X_train[0][:,4]) #y_train[:,:,0]) #, X=[X_train[0][:,[yeardayIdx,5,6]]])\n",
    "\n",
    "class_weight_dict = dict(enumerate(class_weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### use standard scaler for train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize the target\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler0 = StandardScaler()\n",
    "X_train[0] = scaler0.fit_transform(X_train[0])\n",
    "X_test[0]  = scaler0.transform(X_test[0])\n",
    "\n",
    "#scaler1 = StandardScaler()\n",
    "#X_train[1] = scaler1.fit_transform(X_train[1].reshape(-1,1)).reshape(-1,24,1)\n",
    "#X_test[1]  = scaler1.transform(X_test[1].reshape(-1,1)).reshape(-1,24,1)\n",
    "\n",
    "#scaler2 = StandardScaler()\n",
    "#X_train[2] = scaler2.fit_transform(X_train[2])\n",
    "#X_test[2]  = scaler2.transform(X_test[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### use standard scaler for target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize the target\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scalerT = StandardScaler()\n",
    "y_trainAbsN_cons = scalerT.fit_transform(y_trainAbs[:,:,0].reshape(-1, 1)).reshape(-1,24,1)\n",
    "y_testAbsN_cons  = scalerT.transform(y_testAbs[:,:,0].reshape(-1, 1)).reshape(-1,24,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### use a label encoder for the eic count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "eicCountIdx = featConstKeepIdx.index(4)\n",
    "# Assuming cat_feature is a categorical feature\n",
    "encoder = LabelEncoder()\n",
    "constValsArray[:,eicCountIdx] = encoder.fit_transform(constValsArray[:,eicCountIdx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### define model & train it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### transformer net\n",
    "maximum around 140 test error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 24, 3)\n",
      "(None, 24, 1)\n",
      "(None, 109)\n",
      "(None, 109)\n",
      "(None, 109)\n",
      "(None, 109)\n",
      "(None, 109)\n",
      "(None, 109)\n",
      "(None, 109)\n",
      "(None, 109)\n",
      "(None, 109)\n",
      "(None, 109)\n",
      "(None, 109)\n",
      "(None, 109)\n",
      "(None, 109)\n",
      "(None, 109)\n",
      "(None, 109)\n",
      "(None, 109)\n",
      "(None, 109)\n",
      "(None, 109)\n",
      "(None, 109)\n",
      "(None, 109)\n",
      "(None, 109)\n",
      "Epoch 1/20\n",
      "73/73 [==============================] - 36s 391ms/step - loss: 346.6179 - val_loss: 435.0482\n",
      "Epoch 2/20\n",
      "73/73 [==============================] - 27s 369ms/step - loss: 244.3600 - val_loss: 403.2021\n",
      "Epoch 3/20\n",
      "73/73 [==============================] - 28s 380ms/step - loss: 224.2414 - val_loss: 309.0650\n",
      "Epoch 4/20\n",
      "73/73 [==============================] - 29s 392ms/step - loss: 224.9236 - val_loss: 285.6201\n",
      "Epoch 5/20\n",
      "73/73 [==============================] - 31s 419ms/step - loss: 218.6691 - val_loss: 302.0694\n",
      "Epoch 6/20\n",
      "73/73 [==============================] - 28s 386ms/step - loss: 218.5797 - val_loss: 290.9828\n",
      "Epoch 7/20\n",
      "73/73 [==============================] - 30s 416ms/step - loss: 211.8666 - val_loss: 281.2687\n",
      "Epoch 8/20\n",
      "73/73 [==============================] - 29s 403ms/step - loss: 209.7831 - val_loss: 279.4313\n",
      "Epoch 9/20\n",
      "73/73 [==============================] - 32s 439ms/step - loss: 203.4242 - val_loss: 274.7908\n",
      "Epoch 10/20\n",
      "73/73 [==============================] - 29s 394ms/step - loss: 200.4592 - val_loss: 282.4118\n",
      "Epoch 11/20\n",
      "73/73 [==============================] - 29s 404ms/step - loss: 208.4763 - val_loss: 277.0011\n",
      "Epoch 12/20\n",
      "73/73 [==============================] - 31s 426ms/step - loss: 196.2120 - val_loss: 282.7356\n",
      "Epoch 13/20\n",
      "73/73 [==============================] - 30s 405ms/step - loss: 194.0919 - val_loss: 290.1607\n",
      "Epoch 14/20\n",
      "73/73 [==============================] - 29s 395ms/step - loss: 194.3582 - val_loss: 286.0904\n",
      "Epoch 15/20\n",
      "73/73 [==============================] - 29s 396ms/step - loss: 191.0839 - val_loss: 286.6994\n",
      "Epoch 16/20\n",
      "73/73 [==============================] - 29s 399ms/step - loss: 193.7695 - val_loss: 284.2904\n",
      "Epoch 17/20\n",
      "73/73 [==============================] - 29s 400ms/step - loss: 194.2495 - val_loss: 291.9507\n",
      "Epoch 18/20\n",
      "73/73 [==============================] - 29s 395ms/step - loss: 191.1095 - val_loss: 275.9806\n",
      "Epoch 19/20\n",
      "73/73 [==============================] - 29s 396ms/step - loss: 177.8078 - val_loss: 275.2917\n",
      "Epoch 20/20\n",
      "73/73 [==============================] - 31s 430ms/step - loss: 178.4999 - val_loss: 256.3813\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, MultiHeadAttention, LayerNormalization, Flatten\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "weatherPred_inputs3 = tf.keras.Input(shape=(24, nPredFeat), name='weatherPred_inputs2')\n",
    "ePrices_inputs3     = tf.keras.Input(shape=(24, 1), name='ePrices_inputs2')\n",
    "weatherHist_input3  = tf.keras.Input(shape=(24, nHistFeat), name='weatherHist_input2')\n",
    "constant_inputs3    = tf.keras.Input(shape=(nConst,), name='constant_inputs2')\n",
    "\n",
    "oldTargets_input = tf.keras.Input(shape=(24,1), name='oldTargets_input')\n",
    "#oldTargets_input = oldTargets_input[:,:,0] #choose only the cons input\n",
    "#oldTargets_input = tf.expand_dims(oldTargets_input, axis=-1)\n",
    "\n",
    "weatherPred_inputs3_normalized = BatchNormalization()(weatherPred_inputs3)\n",
    "ePrices_inputs3_normalized = BatchNormalization()(ePrices_inputs3)\n",
    "weatherHist_input3_normalized = BatchNormalization()(weatherHist_input3)\n",
    "constant_inputs3_normalized = BatchNormalization()(constant_inputs3)\n",
    "\n",
    "encoder_inputs = tf.concat([weatherPred_inputs3_normalized, ePrices_inputs3_normalized], axis=-1)\n",
    "#encoder_inputs = tf.concat([weatherPred_inputs3_normalized, ePrices_inputs3_normalized, weatherHist_input3_normalized], axis=-1)\n",
    "input_shape = encoder_inputs.shape\n",
    "\n",
    "x = encoder_inputs\n",
    "print(input_shape)\n",
    "for _ in range(8):\n",
    "    x = MultiHeadAttention(num_heads=8, key_dim=16, dropout=0.1)(x, x)\n",
    "    x = LayerNormalization(epsilon=1e-6)(x)\n",
    "    y = x\n",
    "    x = Dense(input_shape[-1], activation=\"relu\")(x)\n",
    "    #x = Dense(24*, activation=\"relu\")(x)\n",
    "    x = tf.keras.layers.Add()([x, y])  # Residual connection\n",
    "    x = Dense(input_shape[-1], activation=\"relu\")(x)\n",
    "\n",
    "# Flatten the encoder output\n",
    "encoder_outputs = Flatten()(x)\n",
    "\n",
    "# Decoder\n",
    "y = oldTargets_input # = decoder input\n",
    "\n",
    "decoder_shape = y.shape\n",
    "print(decoder_shape)\n",
    "for _ in range(8):\n",
    "    y = MultiHeadAttention(num_heads=8, key_dim=16, dropout=0.1)(y, y)\n",
    "    y = LayerNormalization(epsilon=1e-6)(y)\n",
    "    z = y\n",
    "    y = Dense(decoder_shape[-1], activation=\"relu\")(y)\n",
    "    y = tf.keras.layers.Add()([y, z])  # Residual connection\n",
    "    y = Dense(decoder_shape[-1], activation=\"relu\")(y)\n",
    "\n",
    "# Flatten the decoder output\n",
    "decoder_outputs = Flatten()(y)\n",
    "\n",
    "# Concatenate encoder and decoder outputs\n",
    "concatenated_outputs = tf.keras.layers.Concatenate(axis=-1)([encoder_outputs, decoder_outputs, constant_inputs3_normalized])\n",
    "\n",
    "for i in range(20):\n",
    "    concatenated_outputs = Dense(int(concatenated_outputs.shape[-1]), activation=\"relu\")(concatenated_outputs)\n",
    "    print(concatenated_outputs.shape)\n",
    "\n",
    "for i in range(1):\n",
    "    concatenated_outputs = Dense(int(concatenated_outputs.shape[-1]/(i+1)), activation=\"relu\")(concatenated_outputs)\n",
    "    print(concatenated_outputs.shape)\n",
    "final_output = Dense(24, activation=\"relu\")(concatenated_outputs)\n",
    "\n",
    "# Build the model\n",
    "transformer_model = Model(inputs=[constant_inputs3,ePrices_inputs3,weatherPred_inputs3,oldTargets_input], outputs=final_output)\n",
    "#transformer_model = Model(inputs=[constant_inputs3, ePrices_inputs3, weatherHist_input3,weatherPred_inputs3,oldTargets_input], outputs=final_output)\n",
    "\n",
    "\n",
    "# Compile the model\n",
    "transformer_model.compile(optimizer='adam', loss='mae')\n",
    "\n",
    "history3 = transformer_model.fit(X_train, y_trainAbs[:,:,0], validation_data=(X_test, y_testAbs[:,:,0]), epochs=20, batch_size=400)#, class_weight=class_weight_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history3 = transformer_model.fit(X_train, y_trainAbs[:,:,0], validation_data=(X_test, y_testAbs[:,:,0]), epochs=50, batch_size=5000, class_weight=class_weight_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer_model.save('transformer2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "mode": "lines",
         "name": "Train Loss",
         "type": "scatter",
         "x": [
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20
         ],
         "y": [
          346.61785888671875,
          244.36004638671875,
          224.2413787841797,
          224.923583984375,
          218.66909790039062,
          218.57965087890625,
          211.86663818359375,
          209.7830810546875,
          203.42416381835938,
          200.45919799804688,
          208.47633361816406,
          196.21202087402344,
          194.09185791015625,
          194.3582000732422,
          191.08392333984375,
          193.76951599121094,
          194.24951171875,
          191.10946655273438,
          177.80775451660156,
          178.49990844726562
         ]
        },
        {
         "mode": "lines",
         "name": "Validation Loss",
         "type": "scatter",
         "x": [
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20
         ],
         "y": [
          435.0481872558594,
          403.2021484375,
          309.06500244140625,
          285.6201477050781,
          302.0694274902344,
          290.9828186035156,
          281.2686767578125,
          279.43133544921875,
          274.79083251953125,
          282.41180419921875,
          277.0011291503906,
          282.735595703125,
          290.16070556640625,
          286.0904235839844,
          286.69940185546875,
          284.2904357910156,
          291.9506530761719,
          275.9805603027344,
          275.2917175292969,
          256.3812561035156
         ]
        }
       ],
       "layout": {
        "legend": {
         "traceorder": "normal",
         "x": 0,
         "y": 1
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Training and Validation Loss Over Epochs"
        },
        "xaxis": {
         "title": {
          "text": "Epoch"
         }
        },
        "yaxis": {
         "title": {
          "text": "Loss"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = go.Figure()\n",
    "length = len(history3.history['loss'])\n",
    "fig.add_trace(go.Scatter(x=np.arange(1, length+1), y=history3.history['loss'], mode='lines', name='Train Loss'))\n",
    "fig.add_trace(go.Scatter(x=np.arange(1, length+1), y=history3.history['val_loss'], mode='lines', name='Validation Loss'))\n",
    "fig.update_layout(\n",
    "    title='Training and Validation Loss Over Epochs',\n",
    "    xaxis=dict(title='Epoch'),\n",
    "    yaxis=dict(title='Loss'),\n",
    "    legend=dict(x=0, y=1, traceorder='normal'),\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# reconstruct dataframe and analyse results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# restructure dataframes, SHOULD BE CONST IF WE DON'T CHANGE MASK & NOT SHUFFLE\n",
    "storedArr = shuffledArr\n",
    "#storedArr[0]=storedArr[0][:,featConstKeepIdx]\n",
    "\n",
    "appendedList = []\n",
    "shuffledDataBlockId = storedArr[6]\n",
    "for i,data_block_id in enumerate(shuffledDataBlockId):\n",
    "    isTrain = mask[i]\n",
    "    \n",
    "    new_columns = np.zeros((24, 2))  # Adjust the size according to your requirements\n",
    "\n",
    "    #use targets as first block\n",
    "    dataBlock = storedArr[4][i,:,:].reshape(24,2)\n",
    "\n",
    "    #add train/test info & data block id\n",
    "    new_columns = np.ones((24, 2)) * [isTrain, data_block_id]\n",
    "    dataBlock = np.hstack((dataBlock, new_columns))\n",
    "    #add const values\n",
    "    new_columns = np.ones((24, nConst)) * storedArr[0][i,:].reshape(1,storedArr[0].shape[1])\n",
    "    dataBlock = np.hstack((dataBlock, new_columns))\n",
    "    #eprices (of yesterday)\n",
    "    dataBlock = np.hstack((dataBlock, storedArr[1][i,:,:].reshape(24,1)))\n",
    "    # add weather prediction\n",
    "    dataBlock = np.hstack((dataBlock, storedArr[3][i,:,:].reshape(24,storedArr[3].shape[-1])))\n",
    "    # hist weather\n",
    "    dataBlock = np.hstack((dataBlock, storedArr[2][i,:,:].reshape(24,storedArr[2].shape[-1])))\n",
    "\n",
    "    # old target\n",
    "    dataBlock = np.hstack((dataBlock, storedArr[5][i,:].reshape(24,storedArr[5].shape[-1])))\n",
    "\n",
    "    #hist weather, flatten the array and add all values as columns\n",
    "    #new_columns = np.ones((24, 100)) * shuffledArr[2][i,:].reshape(1,100)\n",
    "    #dataBlock = np.hstack((dataBlock, new_columns))\n",
    "\n",
    "    appendedList.append(dataBlock)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "feat = ['y_cons','y_prod','isTrain','data_block_id'] + featConst + ['ePrices'] + featPredWeather + featHistWeather + ['y_consOld','y_prodOld']\n",
    "#for i in range(0,10):\n",
    "#    feat = feat + [f'{value}_'+str(i) for value in featHistWeather]\n",
    "\n",
    "appendedDf = pd.DataFrame(np.vstack(appendedList), columns=feat)\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "appendedDf['unique_time'] = appendedDf.apply(lambda row: datetime(row['year'].astype(int), 1, 1) + timedelta(days=row['yearday'] - 1, hours=row['hours_ahead']-24), axis=1)\n",
    "#appendedDf['unique_time'] = appendedDf.apply(lambda row: datetime(2022, 1, 1) + timedelta(days=row['yearday'] - 1, hours=row['hours_ahead']-24), axis=1)\n",
    "    \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featConstKeepIdx = [0,1,2,5,6,8,9,13,16,17,18]\n",
    "storedArr[0]=storedArr[0][:,featConstKeepIdx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X2 = [storedArr[0],storedArr[1],storedArr[2],storedArr[3],storedArr[5]]\n",
    "#X2 = [storedArr[0],storedArr[1],storedArr[3]]\n",
    "#X2[0]  = scaler0.transform(X2[0])\n",
    "pred_cons = transformer_model.predict(X2)\n",
    "\n",
    "#pred_cons = pred_cons*X2[0][:,CapIdx].reshape(X2[0].shape[0],1)\n",
    "#pred_cons = scalerT.inverse_transform(pred_cons.reshape(-1,1)).reshape(-1,24,1)\n",
    "\n",
    "appendedListProd = []\n",
    "for i in range(0,pred_cons.shape[0]):\n",
    "    appendedListProd.append(pred_cons[i,:].reshape(24,1))\n",
    "    \n",
    "appendedDf['pred_cons'] = np.vstack(appendedListProd)\n",
    "\n",
    "appendedDf['absErr_cons'] = (appendedDf['y_cons']-appendedDf['pred_cons'])\n",
    "print(X2[0].shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# analyze dataframe\n",
    "good generalization, test and train are pretty much always the same\n",
    "\n",
    "production:\n",
    "- overweighting big producers -> lots of errors for smaller capacities\n",
    "\n",
    "consumption:\n",
    "- bigger error in summer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDf = appendedDf.loc[appendedDf.isTrain == True]\n",
    "testDf = appendedDf.loc[appendedDf.isTrain == False]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### investigate consumption\n",
    "- some heavy overestimation (when no consumption but we predict one)\n",
    "- some heave underestimation, when lots of consumption but no prediciton\n",
    "\n",
    "we can't fit yearly trend!!\n",
    "things that don't help:\n",
    "- relu activation function doesn't help (but makes targets all positive)\n",
    "- input normalization\n",
    "- target normalization\n",
    "- bigger network\n",
    "- using less features to predict\n",
    "- using weights on day doesn't help (but is definitely needed)\n",
    "\n",
    "\n",
    "it seems that the consumption data is very sparse & hence really hard to fit \n",
    "-> try to find a normalization criteria\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train\n",
    "print(trainDf.y_cons.mean(),trainDf.y_cons.std())\n",
    "print(trainDf.pred_cons.mean(),trainDf.pred_cons.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test\n",
    "print(testDf.y_cons.mean(),testDf.y_cons.std())\n",
    "print(testDf.pred_cons.mean(),testDf.pred_cons.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDf.groupby('hours_ahead')[['absErr_cons','pred_cons','y_cons']].mean().plot()\n",
    "trainDf.groupby('weekday')[['absErr_cons','pred_cons','y_cons']].mean().plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDf.groupby('unique_time')[['y_cons','absErr_cons','pred_cons']].mean().plot()\n",
    "trainDf.groupby('unique_time')[['y_cons','absErr_cons','pred_cons']].std().plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testDf.groupby('hours_ahead')[['absErr_cons','pred_cons','y_cons']].mean().plot()\n",
    "testDf.groupby('weekday')[['absErr_cons','pred_cons','y_cons']].mean().plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# notice that feb-may in 2023 are way different predictions\n",
    "testDf.groupby('unique_time')[['y_cons','absErr_cons','pred_cons']].mean().plot()\n",
    "testDf.groupby('unique_time')[['y_cons','absErr_cons','pred_cons']].std().plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testDf.loc[(testDf.data_block_id == 351) & (testDf.hours_ahead == 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testDf.loc[testDf.ePrices > 1000].unique_time.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testDf['normCons'] = testDf['y_cons'] / testDf['windspeed_10mint']\n",
    "#testDf['normConsProd'] = testDf['pred_cons'] / testDf['installed_capacity']\n",
    "trainDf = trainDf.sort_values(by='unique_time')\n",
    "a = trainDf.loc[(trainDf.month == 3)]\n",
    "\n",
    "fig = subplots.make_subplots(rows=2, cols=1,shared_xaxes=True)\n",
    "\n",
    "fig.add_trace(go.Scatter(x = a['unique_time'], y = a['y_cons'],mode ='markers', name='y_cons'),row=1, col=1)\n",
    "fig.add_trace(go.Scatter(x = a['unique_time'], y = a['pred_cons'],mode ='markers', name='pred_cons'),row=1, col=1)\n",
    "fig.add_trace(go.Scatter(x = a['unique_time'], y = a['absErr_cons'],mode ='markers', name='err'),row=2, col=1)\n",
    "#fig.add_trace(go.Scatter(x = a['unique_time'], y = a['normConsProd'],mode ='markers', name='normConsProd'),row=1, col=1)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### check individual day difference\n",
    "maybe some features corrupt result, seems that predictions are sometimes far off\n",
    "features that are iffy: eic_count, ind_customer_id, prediction_unit_id, year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "f = ['y_cons','pred_cons','absErr_cons','county',\n",
    " 'is_business',\n",
    " 'product_type',\n",
    "\n",
    " 'installed_capacity',\n",
    " 'business_prodType',\n",
    " #'lowest_price_per_mwh',\n",
    " #'highest_price_per_mwh',\n",
    " #'yearday',\n",
    " #'weekday',\n",
    " #'month',\n",
    " #'monthday'\n",
    " ]\n",
    "a = testDf.loc[(testDf.month == 8) & (testDf.monthday == 3) & (testDf.hours_ahead == 25)][f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = testDf.loc[(testDf.month == 3) & (testDf.monthday == 3) & (testDf.hours_ahead == 25)][f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('Display.max_columns', None)\n",
    "pd.set_option('Display.max_rows', None)\n",
    "c = pd.merge(a,b,on=['county','is_business','product_type','business_prodType'], suffixes=('_a','_b'),how='outer')\n",
    "sorted_columns = sorted(c.columns)\n",
    "c = c[sorted_columns]\n",
    "c\n",
    "#features that are iffy: eic_count, ind_customer_id, prediction_unit_id, year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('Display.max_columns', 10)\n",
    "pd.set_option('Display.max_rows', 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### check error distribution, why we can't generalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDf.absErr_cons.hist(bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testDf.absErr_cons.hist(bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testDf.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualizeDFErrorPDE(df, name,errorBinWidth=500):\n",
    "    b = df.copy()\n",
    "    overallSamples = df.shape[0]\n",
    "    features = b.columns\n",
    "    num_bins = int((b['absErr_cons'].max()-b['absErr_cons'].min())/errorBinWidth)\n",
    "    error_bins = np.linspace(b['absErr_cons'].min(), b['absErr_cons'].max(), num_bins + 1)\n",
    "    b['bin'] = pd.cut(b['absErr_cons'], bins=error_bins)\n",
    "    b['bin'] = b['bin'].apply(lambda x: x.mid)\n",
    "\n",
    "    for i in range(0,len(features)):\n",
    "        fig = go.Figure()\n",
    "        feature_name = features[i]\n",
    "        if feature_name == 'unique_time':\n",
    "            continue\n",
    "        sub = b.iloc[:,i]\n",
    "\n",
    "        uniqueVals = sub.unique()\n",
    "        b['binsFeat'] = sub #if categorical feature\n",
    "    \n",
    "        featBinNr = 32\n",
    "        if len(uniqueVals) > featBinNr:\n",
    "            uniqueVals = np.linspace(sub.min(), sub.max(), featBinNr+1)\n",
    "            b['binsFeat'] = pd.cut(sub, bins=uniqueVals)\n",
    "            b['binsFeat'] = b['binsFeat'].apply(lambda x: x.mid)\n",
    "\n",
    "        binsErr = np.sort(b.bin.unique())\n",
    "        \n",
    "        sampleCountFeat = b['binsFeat'].value_counts().to_dict()\n",
    "        for bin in binsErr:\n",
    "            featCount = b.loc[b.bin == bin]['binsFeat'].value_counts().to_dict()\n",
    "            for key in featCount.keys():\n",
    "                if sampleCountFeat[key] <= 0:\n",
    "                    continue\n",
    "                featCount[key] = featCount[key] / sampleCountFeat[key]\n",
    "            fig.add_trace(go.Bar(x=list(featCount.keys()), y=list(featCount.values()),name=f'Bin_{bin}'))\n",
    "\n",
    "\n",
    "        for key in sampleCountFeat.keys():\n",
    "                sampleCountFeat[key] = sampleCountFeat[key] / overallSamples\n",
    "        fig.add_trace(go.Scatter(x=list(sampleCountFeat.keys()), y=list(sampleCountFeat.values()),mode='markers',name=f'SampleCount'))\n",
    "\n",
    "        fig.update_layout(title=str(feature_name))\n",
    "        # Show the figure\n",
    "        fig.write_html(name+\"pde\"+str(feature_name)+str(i)+\".html\",auto_open=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualizeDFErrorPDE(trainDf, 'trainDf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.13 ('predictenegycons')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "691b0b37d4acdba726dc1c278f9e08b3b5ec9c42dc9e0c737423aa652ed87f03"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
