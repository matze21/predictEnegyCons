{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.interpolate import griddata\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.offline as pyo\n",
    "from plotly import subplots\n",
    "import plotly.graph_objects as go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = pd.read_csv(\"../../data/client.csv\")\n",
    "ePrices = pd.read_csv(\"../../data/electricity_prices.csv\")\n",
    "gasPrices = pd.read_csv(\"../../data/gas_prices.csv\")\n",
    "train = pd.read_csv(\"../../data/train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weatherPredInt = pd.read_csv(\"interpolPredWeather.csv\")\n",
    "weatherHistInt = pd.read_csv(\"../histWeatherSnowCover.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# process data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### add sunlight feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from suntime import Sun\n",
    "import pytz\n",
    "\n",
    "sunrise = []\n",
    "sunset = []\n",
    "for i in range(0,weatherPredInt.shape[0]):\n",
    "    sun = Sun(weatherPredInt.latitude.iloc[i], weatherPredInt.longitude.iloc[i])\n",
    "    dt = pd.to_datetime(weatherPredInt.forecast_datetime.iloc[i])\n",
    "    sunrise.append(sun.get_sunrise_time(dt))\n",
    "    sunset.append(sun.get_sunset_time(dt))\n",
    "\n",
    "#weatherPredInt['sunrise'] = sunrise\n",
    "#weatherPredInt['sunset'] = sunset\n",
    "\n",
    "weatherPredInt['forecast_datetime'] = pd.to_datetime(weatherPredInt['forecast_datetime'])\n",
    "def is_daylight(row):\n",
    "    sunrise = row['sunrise']\n",
    "    sunset = row['sunset']\n",
    "    datetime = row['forecast_datetime']\n",
    "    return sunrise <= datetime <= sunset\n",
    "def calc_daylight(row):\n",
    "    sunrise = row['sunrise']\n",
    "    sunset = row['sunset']\n",
    "    return (sunset-sunrise).total_seconds()/60\n",
    "#weatherPredInt['daylight'] = weatherPredInt.apply(is_daylight, axis=1)\n",
    "#weatherPredInt['minDaylight'] = weatherPredInt.apply(calc_daylight, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### merge client & train, add business feat & interpolate daylight savings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "producing = train.loc[train.is_consumption == 0]\n",
    "consuming = train.loc[train.is_consumption == 1]\n",
    "train = pd.merge(producing.drop('is_consumption',axis = 1), consuming.drop('is_consumption',axis = 1),on=['data_block_id','prediction_unit_id','datetime','county','is_business','product_type'], how='outer',suffixes=('_prod', '_cons'))\n",
    "del producing, consuming\n",
    "print(train.shape)\n",
    "\n",
    "clientsTime = pd.merge(train, client, on=['county','is_business','product_type','data_block_id'], how='inner')\n",
    "\n",
    "clientsTime['datetime'] = pd.to_datetime(clientsTime['datetime'])\n",
    "clientsTime['yearday'] = clientsTime['datetime'].dt.day_of_year\n",
    "clientsTime['weekday'] = clientsTime['datetime'].dt.day_of_week\n",
    "clientsTime['month'] = clientsTime['datetime'].dt.month\n",
    "\n",
    "unique_pairs = list(set(zip(clientsTime['is_business'], clientsTime[ 'product_type'])))\n",
    "pair_index_dict = {pair: index for index, pair in enumerate(unique_pairs)}\n",
    "clientsTime['business_prodType'] = list(map(pair_index_dict.get, zip(clientsTime['is_business'], clientsTime['product_type'])))\n",
    "\n",
    "unique_pairs_cust = list(set(zip(clientsTime['is_business'], clientsTime[ 'product_type'], clientsTime['county'], clientsTime['eic_count'],clientsTime['installed_capacity'])))\n",
    "pair_index_dict = {pair: index for index, pair in enumerate(unique_pairs_cust)}\n",
    "clientsTime['ind_customer_id'] = list(map(pair_index_dict.get, zip(clientsTime['is_business'], clientsTime['product_type'], clientsTime['county'], clientsTime['eic_count'],clientsTime['installed_capacity'])))\n",
    "\n",
    "# interpolate daylight savings\n",
    "clientsTime = clientsTime.interpolate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### prepare electric prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ePrices['forecast_date'] = pd.to_datetime(ePrices['forecast_date'])\n",
    "\n",
    "# Set 'timestamp' as the index\n",
    "ePrices.set_index('forecast_date', inplace=True)\n",
    "\n",
    "# Resample to fill missing hours\n",
    "df_resampled = ePrices.resample('1H').asfreq()\n",
    "\n",
    "# Linearly interpolate missing values\n",
    "ePrices = df_resampled.interpolate(method='linear')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# write data into python arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clientsTime, weatherPredInt, weatherHistInt, ePrices, gasPrices\n",
    "\n",
    "featPredWeather = [\n",
    "        #'latitude', 'longitude', \n",
    "        'County', #'forecast_datetime',\n",
    "       'hours_ahead',\n",
    "        'data_block_id', #'origin_datetime', \n",
    "       'temperatureint',\n",
    "       'dewpointint', 'cloudcover_highint', 'cloudcover_lowint',\n",
    "       'cloudcover_midint', 'cloudcover_totalint',\n",
    "       '10_metre_u_wind_componentint', '10_metre_v_wind_componentint',\n",
    "       'direct_solar_radiationint', 'surface_solar_radiation_downwardsint',\n",
    "       'snowfallint', 'total_precipitationint', \n",
    "       #'sunrise', 'sunset',\n",
    "       #'daylight', 'minDaylight'\n",
    "       ]\n",
    "\n",
    "predWeatherComplete = weatherPredInt[featPredWeather]\n",
    "del weatherPredInt\n",
    "\n",
    "featHistWeather = [\n",
    "       #'latitude', 'longitude', \n",
    "       'County',\n",
    "       #'datetime', \n",
    "       'data_block_id', \n",
    "       'temperatureint', 'dewpointint', 'rainint',\n",
    "       'snowfallint', 'surface_pressureint', 'cloudcover_totalint',\n",
    "       'cloudcover_lowint', 'cloudcover_midint', 'cloudcover_highint',\n",
    "       'windspeed_10mint', 'winddirection_10mint', 'shortwave_radiationint',\n",
    "       'direct_solar_radiationint', 'diffuse_radiationint', 'meltingInCM',\n",
    "       'snowHeightFlux', 'snowcover'\n",
    "]\n",
    "histWeatherComplete = weatherHistInt[featHistWeather]\n",
    "del weatherHistInt\n",
    "\n",
    "# can't use client id as an axis in data because it's not constant!\n",
    "y = np.zeros((clientsTime.shape[0],2))\n",
    "y_indexes = np.zeros((clientsTime.shape[0],2)) # dataframe indexes in the end, not important for now\n",
    "index_y = 0\n",
    "\n",
    "customers = []\n",
    "\n",
    "constValsArray = np.zeros((1,10))\n",
    "targetsArray = np.zeros((1,24,2))\n",
    "ePricesArray = np.zeros((1,24,1))\n",
    "\n",
    "nFeatHistWeather = histWeatherComplete.shape[1]-1\n",
    "histWeatherArray = np.zeros((1,24,nFeatHistWeather))\n",
    "nFeatPredWeather = predWeatherComplete.shape[1]-1\n",
    "predWeatherArray = np.zeros((1,24,nFeatPredWeather))\n",
    "dataBlockIdArray = np.zeros((1))\n",
    "\n",
    "# loop over customer, append all data cycles for each customer\n",
    "for customerId in clientsTime.ind_customer_id.unique():\n",
    "    customerSlice = clientsTime.loc[clientsTime.ind_customer_id == customerId]\n",
    "\n",
    "    # const values\n",
    "    county       = customerSlice.county.unique()[0]\n",
    "    is_business  = customerSlice.is_business.unique()[0]\n",
    "    product_type = customerSlice.product_type.unique()[0]\n",
    "    prediction_unit_id = customerSlice.prediction_unit_id.unique()[0] #should be redundant\n",
    "    eic_count          = customerSlice.eic_count.unique()[0]\n",
    "    installed_capacity = customerSlice.installed_capacity.unique()[0]\n",
    "    yearday = customerSlice.yearday.unique()[0]\n",
    "    weekday = customerSlice.weekday.unique()[0]\n",
    "    month   = customerSlice.month.unique()[0]\n",
    "    business_prodType = customerSlice.business_prodType.unique()[0]\n",
    "    ind_customer_id   = customerSlice.ind_customer_id.unique()[0]\n",
    "\n",
    "    for dataBlockId in customerSlice.data_block_id.unique():\n",
    "        timeSlice = customerSlice.loc[customerSlice.data_block_id == dataBlockId]\n",
    "        gasSlice = gasPrices.loc[gasPrices.data_block_id == dataBlockId]\n",
    "        eSlice   = ePrices.loc[ePrices.data_block_id == dataBlockId]\n",
    "\n",
    "        \n",
    "\n",
    "        lowest_price_per_mwh = gasSlice.lowest_price_per_mwh.iloc[0]\n",
    "        highest_price_per_mwh = gasSlice.highest_price_per_mwh.iloc[0]\n",
    "        euros_per_mwh = eSlice['euros_per_mwh'].to_numpy()\n",
    "        if euros_per_mwh.shape[0] == 23:\n",
    "            print(dataBlockId, customerId)\n",
    "\n",
    "        y_cons = timeSlice['target_cons'] / installed_capacity\n",
    "        y_prod = timeSlice['target_prod'] / installed_capacity\n",
    "\n",
    "        histWeather = histWeatherComplete.loc[(histWeatherComplete.data_block_id == dataBlockId) & (histWeatherComplete.County == county)]\n",
    "        predWeather = predWeatherComplete.loc[(predWeatherComplete.data_block_id == dataBlockId) & (predWeatherComplete.County == county)]\n",
    "\n",
    "        histWeather = histWeather.drop('data_block_id', axis = 1)\n",
    "        predWeather = predWeather.drop('data_block_id', axis = 1)\n",
    "\n",
    "\n",
    "        if y_cons.shape[0] == 23:\n",
    "            print(dataBlockId, customerId)\n",
    "\n",
    "        new_row = np.array([county, is_business, product_type, prediction_unit_id, eic_count, installed_capacity, business_prodType, ind_customer_id,lowest_price_per_mwh,highest_price_per_mwh])\n",
    "        constValsArray = np.vstack((constValsArray, new_row))\n",
    "\n",
    "        new_row = np.zeros((1,24,2))\n",
    "        new_row[:,:,0] = y_cons\n",
    "        new_row[:,:,1] = y_prod\n",
    "        targetsArray = np.concatenate((targetsArray, new_row), axis=0)\n",
    "\n",
    "        new_row = np.zeros((1,24,1))\n",
    "        new_row[0,:,0] = euros_per_mwh\n",
    "        ePricesArray = np.concatenate((ePricesArray, new_row), axis=0)\n",
    "\n",
    "        new_row = np.zeros((1,24,nFeatHistWeather)) #without datablock id\n",
    "        new_row[:,:,:] = histWeather\n",
    "        histWeatherArray = np.concatenate((histWeatherArray, new_row), axis=0)\n",
    "\n",
    "        new_row = np.zeros((1,24,nFeatPredWeather)) # withotu datablock id\n",
    "        new_row[:,:,:] = predWeather\n",
    "        predWeatherArray = np.concatenate((predWeatherArray, new_row), axis=0)\n",
    "\n",
    "        dataBlockIdArray = np.concatenate((dataBlockIdArray, np.array([dataBlockId])))\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove first rows\n",
    "targetsArray     = targetsArray[1:,:,:]\n",
    "constValsArray   = constValsArray[1:,:]\n",
    "ePricesArray     = ePricesArray[1:,:,:]\n",
    "histWeatherArray = histWeatherArray[1:,:,:]\n",
    "predWeatherArray = predWeatherArray[1:,:,:]\n",
    "dataBlockIdArray = dataBlockIdArray[1:]\n",
    "print(targetsArray.shape, constValsArray.shape, ePricesArray.shape,histWeatherArray.shape, predWeatherArray.shape, dataBlockIdArray.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calc absolute targets, not normalized by capacity\n",
    "targetsAbs = targetsArray.copy()\n",
    "targetsAbs = targetsAbs * constValsArray[:,5].reshape(constValsArray.shape[0],1,1)\n",
    "print(targetsAbs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(targetsArray.shape, constValsArray.shape, ePricesArray.shape,histWeatherArray.shape, predWeatherArray.shape, dataBlockIdArray.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez('data_arrays_snowcover_GasPr_noSunrise.npz', arr1=targetsArray, arr2=constValsArray, arr3=ePricesArray, arr4 = histWeatherArray, arr5=predWeatherArray, arr6=dataBlockIdArray, arr7=targetsAbs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_data = np.load('data_arrays_snowcover_GasPr_noSunrise.npz')\n",
    "\n",
    "# Access individual arrays by their keys\n",
    "targetsArray = loaded_data['arr1']\n",
    "constValsArray = loaded_data['arr2']\n",
    "ePricesArray = loaded_data['arr3']\n",
    "histWeatherArray = loaded_data['arr4']\n",
    "predWeatherArray = loaded_data['arr5']\n",
    "dataBlockIdArray = loaded_data['arr6']\n",
    "targetsAbs = loaded_data['arr7']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# build a model & train it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### shuffle if needed & split into x train y train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "index_array = np.arange(targetsArray.shape[0])\n",
    "np.random.shuffle(index_array)\n",
    "arrays = [constValsArray,ePricesArray,histWeatherArray,predWeatherArray,targetsAbs,targetsArray,dataBlockIdArray]\n",
    "shuffledArr = [arr[index_array] for arr in arrays]\n",
    "del arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainSplit = int(shuffledArr[4].shape[0]*0.8)\n",
    "X_train = [shuffledArr[0][0:trainSplit,:],shuffledArr[1][0:trainSplit,:,:],shuffledArr[2][0:trainSplit,:,:],shuffledArr[3][0:trainSplit,:,:]]\n",
    "y_trainAbs = shuffledArr[4][0:trainSplit,:]\n",
    "y_train = shuffledArr[5][0:trainSplit,:]\n",
    "\n",
    "X_test = [shuffledArr[0][trainSplit:-1,:],shuffledArr[1][trainSplit:-1,:,:],shuffledArr[2][trainSplit:-1,:,:],shuffledArr[3][trainSplit:-1,:,:]]\n",
    "y_testAbs = shuffledArr[4][trainSplit:-1,:]\n",
    "y_test = shuffledArr[5][trainSplit:-1,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del targetsArray ,constValsArray   ,ePricesArray     ,histWeatherArray ,predWeatherArray ,dataBlockIdArray ,targetsAbs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### split unshuffled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainSplit = int(targetsArray.shape[0]*0.3)\n",
    "X_train = [constValsArray[0:trainSplit,:],ePricesArray[0:trainSplit,:,:],histWeatherArray[0:trainSplit,:,:],predWeatherArray[0:trainSplit,:,:]]\n",
    "y_train = targetsArray[0:trainSplit,:]\n",
    "y_trainAbs = targetsAbs[0:trainSplit,:]\n",
    "\n",
    "X_test = [constValsArray[trainSplit:-1,:],ePricesArray[trainSplit:-1,:,:],histWeatherArray[trainSplit:-1,:,:],predWeatherArray[trainSplit:-1,:,:]]\n",
    "y_test = targetsArray[trainSplit:-1,:]\n",
    "y_testAbs = targetsAbs[trainSplit:-1,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### define model & train it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### model with 2 outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the inputs\n",
    "weatherPred_inputs = tf.keras.Input(shape=(24, 16), name='weatherPred_inputs')\n",
    "ePrices_inputs = tf.keras.Input(shape=(24, 1), name='ePrices_inputs')\n",
    "\n",
    "weatherHist_input = tf.keras.Input(shape=(24, 18), name='weatherHist_input')\n",
    "constant_inputs = tf.keras.Input(shape=(8,), name='constant_inputs')\n",
    "\n",
    "# Process the time-based inputs\n",
    "weatherPred_flattened = layers.Flatten()(weatherPred_inputs)\n",
    "weatherPred_flattened = layers.Dense(408, activation='relu')(weatherPred_flattened)\n",
    "weatherPred_flattened = layers.Dense(408, activation='relu')(weatherPred_flattened)\n",
    "weatherPred_flattened = layers.Dense(408, activation='relu')(weatherPred_flattened)\n",
    "#weatherPred_flattened = layers.Dense(256, activation='relu')(weatherPred_flattened)\n",
    "#weatherPred_flattened = layers.Dense(64, activation='relu')(weatherPred_flattened)\n",
    "\n",
    "weatherHist_flattened = layers.Flatten()(weatherHist_input)\n",
    "weatherHist_flattened = layers.Dense(240, activation='relu')(weatherHist_flattened)\n",
    "weatherHist_flattened = layers.Dense(240, activation='relu')(weatherHist_flattened)\n",
    "weatherHist_flattened = layers.Dense(240, activation='relu')(weatherHist_flattened)\n",
    "#weatherHist_flattened = layers.Dense(64, activation='relu')(weatherHist_flattened)\n",
    "\n",
    "ePrices_layer = layers.Flatten()(ePrices_inputs)\n",
    "ePrices_layer = layers.Dense(24, activation='relu')(ePrices_layer)\n",
    "\n",
    "# Concatenate all inputs\n",
    "x = layers.Concatenate()([weatherPred_flattened, weatherHist_flattened, ePrices_layer, constant_inputs])\n",
    "\n",
    "# Main dense block\n",
    "for i in range(0,5):\n",
    "    x = layers.Dense(540, activation='relu')(x)\n",
    "    #x = layers.Dropout(0.1)(x)\n",
    "\n",
    "# Output layer for 24*2 targets\n",
    "output_layer = layers.Dense(24 * 2, activation='linear', name='output')(x)\n",
    "output_layer = layers.Reshape((24, 2))(output_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Model(inputs=[constant_inputs,ePrices_inputs, weatherHist_input, weatherPred_inputs], outputs=output_layer)\n",
    "\n",
    "# Compile the model with an appropriate loss function and optimizer\n",
    "model.compile(optimizer='adam', loss='mae', metrics=['mae','mse'])\n",
    "\n",
    "history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=200, batch_size=200) #batch size = 110 weather station"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "length = len(history.history['loss'])\n",
    "fig.add_trace(go.Scatter(x=np.arange(1, length+1), y=history.history['loss'], mode='lines', name='Train Loss'))\n",
    "fig.add_trace(go.Scatter(x=np.arange(1, length+1), y=history.history['val_loss'], mode='lines', name='Validation Loss'))\n",
    "fig.update_layout(\n",
    "    title='Training and Validation Loss Over Epochs',\n",
    "    xaxis=dict(title='Epoch'),\n",
    "    yaxis=dict(title='Loss'),\n",
    "    legend=dict(x=0, y=1, traceorder='normal'),\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = model.predict(X_test)\n",
    "predVals = a*X_test[0][:,5].reshape(X_test[0].shape[0],1,1)\n",
    "print(np.mean(np.abs(y_testAbs - predVals)))\n",
    "a = model.predict(X_train)\n",
    "predVals = a*X_train[0][:,5].reshape(X_train[0].shape[0],1,1)\n",
    "trueVals = targetsAbs[0:trainSplit,:]\n",
    "print(np.mean(np.abs(y_trainAbs - predVals)))\n",
    "del a,predVals,trueVals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### use a model for production and consumption separately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the inputs\n",
    "weatherPred_inputs1 = tf.keras.Input(shape=(24, 14), name='weatherPred_inputs1')\n",
    "ePrices_inputs1 = tf.keras.Input(shape=(24, 1), name='ePrices_inputs1')\n",
    "\n",
    "weatherHist_input1 = tf.keras.Input(shape=(24, 18), name='weatherHist_input1')\n",
    "constant_inputs1 = tf.keras.Input(shape=(10,), name='constant_inputs1')\n",
    "\n",
    "# Process the time-based inputs\n",
    "weatherPred_flattened1 = layers.Flatten()(weatherPred_inputs1)\n",
    "for i in range(0,3):\n",
    "    weatherPred_flattened1 = layers.Dense(408, activation='relu')(weatherPred_flattened1)\n",
    "\n",
    "weatherHist_flattened1 = layers.Flatten()(weatherHist_input1)\n",
    "for i in range(0,3):\n",
    "    weatherHist_flattened1 = layers.Dense(240, activation='relu')(weatherHist_flattened1)\n",
    "\n",
    "ePrices_layer1 = layers.Flatten()(ePrices_inputs1)\n",
    "ePrices_layer1 = layers.Dense(24, activation='relu')(ePrices_layer1)\n",
    "\n",
    "# Concatenate all inputs\n",
    "x1 = layers.Concatenate()([weatherPred_flattened1, weatherHist_flattened1, ePrices_layer1, constant_inputs1])\n",
    "\n",
    "# Main dense block\n",
    "for i in range(0,5):\n",
    "    x1 = layers.Dense(540, activation='relu')(x1)\n",
    "    #x = layers.Dropout(0.1)(x)\n",
    "\n",
    "# Output layer for 24*2 targets\n",
    "output_layer1 = layers.Dense(24 * 1, activation='linear', name='output1')(x1)\n",
    "output_layer1 = layers.Reshape((24, 1))(output_layer1)\n",
    "\n",
    "model2 = models.Model(inputs=[constant_inputs1,ePrices_inputs1, weatherHist_input1, weatherPred_inputs1], outputs=output_layer1)\n",
    "\n",
    "# Compile the model with an appropriate loss function and optimizer\n",
    "model2.compile(optimizer='adam', loss='mae', metrics=['mae'])\n",
    "# 0 = cons, 1 = prod\n",
    "history2 = model2.fit(X_train, y_trainAbs[:,:,1], validation_data=(X_test, y_testAbs[:,:,1]), epochs=100, batch_size=400) #batch size = 110 weather station"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the inputs\n",
    "weatherPred_inputs2 = tf.keras.Input(shape=(24, 14), name='weatherPred_inputs2')\n",
    "ePrices_inputs2 = tf.keras.Input(shape=(24, 1), name='ePrices_inputs2')\n",
    "\n",
    "weatherHist_input2 = tf.keras.Input(shape=(24, 18), name='weatherHist_input2')\n",
    "constant_inputs2 = tf.keras.Input(shape=(10,), name='constant_inputs2')\n",
    "\n",
    "# Process the time-based inputs\n",
    "weatherPred_flattened2 = layers.Flatten()(weatherPred_inputs2)\n",
    "for i in range(0,3):\n",
    "    weatherPred_flattened2 = layers.Dense(408, activation='relu')(weatherPred_flattened2)\n",
    "\n",
    "weatherHist_flattened2 = layers.Flatten()(weatherHist_input2)\n",
    "for i in range(0,3):\n",
    "    weatherHist_flattened2 = layers.Dense(240, activation='relu')(weatherHist_flattened2)\n",
    "\n",
    "ePrices_layer2 = layers.Flatten()(ePrices_inputs2)\n",
    "ePrices_layer2 = layers.Dense(24, activation='relu')(ePrices_layer2)\n",
    "\n",
    "# Concatenate all inputs\n",
    "x2 = layers.Concatenate()([weatherPred_flattened2, weatherHist_flattened2, ePrices_layer2, constant_inputs2])\n",
    "\n",
    "# Main dense block\n",
    "for i in range(0,5):\n",
    "    x2 = layers.Dense(540, activation='relu')(x2)\n",
    "    #x = layers.Dropout(0.1)(x)\n",
    "\n",
    "# Output layer for 24*2 targets\n",
    "output_layer2 = layers.Dense(24 * 1, activation='linear', name='output')(x2)\n",
    "output_layer2 = layers.Reshape((24, 1))(output_layer2)\n",
    "\n",
    "model3 = tf.keras.Model(inputs=[constant_inputs2,ePrices_inputs2, weatherHist_input2, weatherPred_inputs2], outputs=output_layer2)\n",
    "\n",
    "# Compile the model with an appropriate loss function and optimizer\n",
    "model3.compile(optimizer='adam', loss='mae', metrics=['mae'])\n",
    "# 0 = cons, 1 = prod\n",
    "history3 = model3.fit(X_train, y_trainAbs[:,:,0], validation_data=(X_test, y_testAbs[:,:,0]), epochs=100, batch_size=200) #batch size = 110 weather station"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "length = len(history3.history['loss'])\n",
    "fig.add_trace(go.Scatter(x=np.arange(1, length+1), y=history3.history['loss'], mode='lines', name='Train Loss'))\n",
    "fig.add_trace(go.Scatter(x=np.arange(1, length+1), y=history3.history['val_loss'], mode='lines', name='Validation Loss'))\n",
    "fig.update_layout(\n",
    "    title='Training and Validation Loss Over Epochs',\n",
    "    xaxis=dict(title='Epoch'),\n",
    "    yaxis=dict(title='Loss'),\n",
    "    legend=dict(x=0, y=1, traceorder='normal'),\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = model2.predict(X_test)\n",
    "predVals = a*X_test[0][:,5].reshape(X_test[0].shape[0],1,1)\n",
    "print(np.mean(np.abs(y_testAbs - predVals)))\n",
    "a = model2.predict(X_train)\n",
    "predVals = a*X_train[0][:,5].reshape(X_train[0].shape[0],1,1)\n",
    "trueVals = targetsAbs[0:trainSplit,:]\n",
    "print(np.mean(np.abs(y_trainAbs - predVals)))\n",
    "del a,predVals,trueVals\n",
    "\n",
    "# mean cons error combined for a relative train = 770\n",
    "# mean cons error single trained (relative train) = 290\n",
    "# mean cons error single traines =43 (train) 60-63 (test)\n",
    "# mean prod error single trainer = 9-20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.save('prod_model_GasPr_noSunrise.h5')\n",
    "model3.save('cons_model_GasPr_noSunrise.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# reconstruct dataframe and analyse results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# restructure dataframes\n",
    "appendedList = []\n",
    "shuffledDataBlockId = shuffledArr[6]\n",
    "for i,data_block_id in enumerate(shuffledDataBlockId):\n",
    "    isTrain = False\n",
    "    if i<trainSplit:\n",
    "        isTrain=True\n",
    "    \n",
    "    new_columns = np.zeros((24, 2))  # Adjust the size according to your requirements\n",
    "\n",
    "    #use targets as first block\n",
    "    dataBlock = shuffledArr[4][i,:,:].reshape(24,2)\n",
    "\n",
    "    #add train/test info & data block id\n",
    "    new_columns = np.ones((24, 2)) * [isTrain, data_block_id]\n",
    "    dataBlock = np.hstack((dataBlock, new_columns))\n",
    "    #add const values\n",
    "    new_columns = np.ones((24, 8)) * shuffledArr[0][i,:].reshape(1,8)\n",
    "    dataBlock = np.hstack((dataBlock, new_columns))\n",
    "    #eprices (of yesterday)\n",
    "    dataBlock = np.hstack((dataBlock, shuffledArr[1][i,:,:].reshape(24,1)))\n",
    "    # add weather prediction\n",
    "    dataBlock = np.hstack((dataBlock, shuffledArr[3][i,:,:].reshape(24,16)))\n",
    "    # hist weather\n",
    "    dataBlock = np.hstack((dataBlock, shuffledArr[2][i,:,:].reshape(24,18)))\n",
    "\n",
    "    #hist weather, flatten the array and add all values as columns\n",
    "    #new_columns = np.ones((24, 100)) * shuffledArr[2][i,:].reshape(1,100)\n",
    "    #dataBlock = np.hstack((dataBlock, new_columns))\n",
    "\n",
    "    appendedList.append(dataBlock)\n",
    "\n",
    "\n",
    "\n",
    "featPredWeather = [\n",
    "        #'latitude', 'longitude', \n",
    "        'County', #'forecast_datetime',\n",
    "       'hours_ahead',\n",
    "        #'data_block_id_predWeather', #'origin_datetime', \n",
    "       'temperatureint',\n",
    "       'dewpointint', 'cloudcover_highint', 'cloudcover_lowint',\n",
    "       'cloudcover_midint', 'cloudcover_totalint',\n",
    "       '10_metre_u_wind_componentint', '10_metre_v_wind_componentint',\n",
    "       'direct_solar_radiationint', 'surface_solar_radiation_downwardsint',\n",
    "       'snowfallint', 'total_precipitationint', \n",
    "       #'sunrise', 'sunset',\n",
    "       'daylight', 'minDaylight']\n",
    "\n",
    "featHistWeather = [\n",
    "       #'County', #'forecast_datetime',\n",
    "       #'hours_ahead', #'data_block_id_histWeather',# 'origin_datetime',\n",
    "       #'diff_dewpoint', 'diff_cloudcover_total', 'diff_direct_solar_radiation',\n",
    "       #'diff_surface_solar_radiation_downwards', 'diff_snowfall',\n",
    "       #'diff_10_metre_u_wind_component', 'diff_10_metre_v_wind_component'\n",
    "       'County',\n",
    "       #'datetime', \n",
    "       #'data_block_id', \n",
    "       'temperatureint', 'dewpointint', 'rainint',\n",
    "       'snowfallint', 'surface_pressureint', 'cloudcover_totalint',\n",
    "       'cloudcover_lowint', 'cloudcover_midint', 'cloudcover_highint',\n",
    "       'windspeed_10mint', 'winddirection_10mint', 'shortwave_radiationint',\n",
    "       'direct_solar_radiationint', 'diffuse_radiationint', 'meltingInCM',\n",
    "       'snowHeightFlux', 'snowcover'\n",
    "]\n",
    "featConst =['county', 'is_business', 'product_type', 'prediction_unit_id', 'eic_count', 'installed_capacity', 'business_prodType', 'ind_customer_id']\n",
    "\n",
    "feat = ['y_cons','y_prod','isTrain','data_block_id'] + featConst + ['ePrices'] + featPredWeather +featHistWeather\n",
    "#for i in range(0,10):\n",
    "#    feat = feat + [f'{value}_'+str(i) for value in featHistWeather]\n",
    "\n",
    "appendedDf = pd.DataFrame(np.vstack(appendedList), columns=feat)\n",
    "    \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "appendedDf.to_csv('appendedDf_snowcover_relativeTrain.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "appendedDf = pd.read_csv('appendedDf_snowcover_relativeTrain.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X2 = [shuffledArr[0],shuffledArr[1],shuffledArr[2],shuffledArr[3]]\n",
    "pred_prod = model2.predict(X2)\n",
    "pred_cons = model3.predict(X2)\n",
    "\n",
    "appendedListCons = []\n",
    "for i in range(0,pred_cons.shape[0]):\n",
    "    appendedListCons.append(pred_cons[i,:,:].reshape(24,1))\n",
    "predDfc = pd.DataFrame(np.vstack(appendedListCons), columns=['pred_cons'])\n",
    "\n",
    "\n",
    "appendedListProd = []\n",
    "for i in range(0,pred_prod.shape[0]):\n",
    "    appendedListProd.append(pred_prod[i,:,:].reshape(24,1))\n",
    "predDfp = pd.DataFrame(np.vstack(appendedListProd), columns=['pred_prod'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del X_train, X_test,y_train,y_trainAbs,y_test,y_testAbs,loaded_data#,shuffledArr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "appendedList = []\n",
    "for ind in range(0,pred.shape[0]):\n",
    "    appendedList.append(pred[i,:,:].reshape(24,2))\n",
    "predDf = pd.DataFrame(np.vstack(appendedList), columns=['pred_cons','pred_prod'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "appendedDf = pd.concat([appendedDf, predDfc], axis = 1)\n",
    "appendedDf = pd.concat([appendedDf, predDfp], axis = 1)\n",
    "appendedDf.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "appendedDf['absErr_cons'] = (appendedDf['y_cons']-appendedDf['pred_cons'])\n",
    "appendedDf['absErr_prod'] = (appendedDf['y_prod']-appendedDf['pred_prod'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# analyze dataframe\n",
    "good generalization, test and train are pretty much always the same\n",
    "\n",
    "production:\n",
    "- overweighting big producers -> lots of errors for smaller capacities\n",
    "\n",
    "consumption:\n",
    "- bigger error in summer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDf = appendedDf.loc[appendedDf.isTrain == True]\n",
    "testDf = appendedDf.loc[appendedDf.isTrain == False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "appendedDf['consTarg-Cap'] = appendedDf['y_cons'] - appendedDf['installed_capacity']\n",
    "appendedDf['prodTarg-Cap'] = appendedDf['y_prod'] - appendedDf['installed_capacity']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "appendedDf['y_prod'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testDf.absErr_cons.hist(bins=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testDf.absErr_prod.hist(bins=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDf.absErr_cons.hist(bins=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDf.absErr_prod.hist(bins=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### investigate consumption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testDf.groupby('data_block_id')['absErr_cons'].mean().plot() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abs(testDf.absErr_prod).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testDf.absErr_cons.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testDf.groupby('minDaylight')['absErr_prod'].max().plot() #hist(bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testDf.loc[testDf.absErr_prod <100].data_block_id.hist(bins=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = subplots.make_subplots(rows=1, cols=1,shared_xaxes=True)\n",
    "fig.add_trace(go.Scatter(x = testDf['data_block_id'], y = testDf['absErr_cons'],mode ='markers', name='diff_snowfall'),row=1, col=1)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.13 ('predictenegycons')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "691b0b37d4acdba726dc1c278f9e08b3b5ec9c42dc9e0c737423aa652ed87f03"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
